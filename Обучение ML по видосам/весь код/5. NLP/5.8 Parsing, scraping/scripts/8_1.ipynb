{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "\n",
    "stem = Mystem()\n",
    "stopwords_extension = ['руб', 'евро', 'долл', 'млн', 'млрд', 'это', 'вице', 'эль']\n",
    "reg = re.compile('[А-Яа-яA-Za-z]+')\n",
    "\n",
    "\n",
    "class ArticleMeta:\n",
    "    # Initializing URL, publish date, authors, tokenized sentences\n",
    "    def __init__(self, url, pub_date, authors, title, lead):\n",
    "        self.url = url\n",
    "        self.pub_date = pub_date\n",
    "        self.authors = authors\n",
    "        self.title = title\n",
    "        self.lead = lead\n",
    "        self.sentences, self.tokens = self.lemmatize(lead)\n",
    "        self.analysis = self.extract_analysis()\n",
    "\n",
    "    # TODO: SyntaxNet tree\n",
    "    @staticmethod\n",
    "    # Preprocess of given lead\n",
    "    def lemmatize(text):\n",
    "        tokens = [token for token in re.findall(reg, text.replace('-', '')) if token not in stopwords.words('russian') +\n",
    "                         stopwords_extension]\n",
    "        text = ' '.join(tokens)\n",
    "        lem_text = ''.join(stem.lemmatize(text))\n",
    "        return lem_text.replace('руб.', 'руб').replace('\\xa0', ''), tokens\n",
    "\n",
    "    def extract_analysis(self):\n",
    "        string_analysis = ''\n",
    "        for token in self.tokens:\n",
    "            if 'analysis' in stem.analyze(token)[0] and len(stem.analyze(token)[0]['analysis']):\n",
    "                string_analysis += stem.analyze(token)[0]['analysis'][0]['gr']\n",
    "                string_analysis += ' '\n",
    "            elif token == '.':\n",
    "                string_analysis += token\n",
    "                string_analysis += ' '\n",
    "        return string_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import urllib.request\n",
    "\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Regular expressions for news lead and publish date\n",
    "reg_html = re.compile('<[^>]+>')\n",
    "calendar = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "calendar_leap = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "\n",
    "\n",
    "# Scraping rubrics and constructing ArticleMeta object\n",
    "def gen_urls(year):\n",
    "    urls = []\n",
    "    for month in range(1): # for speed\n",
    "        if year in [2017]: # for speed\n",
    "            if month < 9:\n",
    "                for day in range(1, calendar_leap[month] + 1):\n",
    "                    if day < 10:\n",
    "                        urls.append('https://www.kommersant.ru/archive/rubric/40/' + str(year) + '-0' + str(month + 1) +\n",
    "                                    '-0' + str(day))\n",
    "                    else:\n",
    "                        urls.append('https://www.kommersant.ru/archive/rubric/40/' + str(year) + '-0' + str(month + 1) +\n",
    "                                    '-' + str(day))\n",
    "            else:\n",
    "                for day in range(1, calendar_leap[month] + 1):\n",
    "                    if day < 10:\n",
    "                        urls.append('https://www.kommersant.ru/archive/rubric/40/' + str(year) + '-' + str(month + 1) +\n",
    "                                    '-0' + str(day))\n",
    "                    else:\n",
    "                        urls.append('https://www.kommersant.ru/archive/rubric/40/' + str(year) + '-' + str(month + 1) +\n",
    "                                    '-' + str(day))\n",
    "        else:\n",
    "            if month < 9:\n",
    "                for day in range(1, calendar[month] + 1):\n",
    "                    if day < 10:\n",
    "                        urls.append('https://www.kommersant.ru/archive/rubric/40/' + str(year) + '-0' + str(month + 1) +\n",
    "                                    '-0' + str(day))\n",
    "                    else:\n",
    "                        urls.append('https://www.kommersant.ru/archive/rubric/40/' + str(year) + '-0' + str(month + 1) +\n",
    "                                    '-' + str(day))\n",
    "            else:\n",
    "                for day in range(1, calendar[month] + 1):\n",
    "                    if day < 10:\n",
    "                        urls.append('https://www.kommersant.ru/archive/rubric/40/' + str(year) + '-' + str(month + 1) +\n",
    "                                    '-0' + str(day))\n",
    "                    else:\n",
    "                        urls.append('https://www.kommersant.ru/archive/rubric/40/' + str(year) + '-' + str(month + 1) +\n",
    "                                    '-' + str(day))\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def scrape_archive(year):\n",
    "    urls = gen_urls(year)\n",
    "    metas = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            req = urllib.request.Request(url, headers={'User-Agent': \"Magic Browser\"})\n",
    "            html = urllib.request.urlopen(url)\n",
    "            html = html.read()\n",
    "            articles = Selector(text=html).xpath('/html/body/div[@class=\"layout\"]/div[@class=\"col_group\"]/'\n",
    "                                                 'div[@class=\"col col-large\"]/div[@class=\"col col-big\"]/'\n",
    "                                                 'section[@class=\"b-other_docs\"]/'\n",
    "                                                 'div[@class=\"b-other_docs__list\"]/a/@href').extract()\n",
    "            for article in articles:\n",
    "                url_art = 'https://www.kommersant.ru' + article\n",
    "                try:\n",
    "                    req = urllib.request.Request(url_art, headers={'User-Agent': \"Magic Browser\"})\n",
    "                    html = urllib.request.urlopen(req).read()\n",
    "                    html_response = HtmlResponse(url_art, body=html)\n",
    "                    time = Selector(html_response).xpath('/html/body/div[@class=\"layout\"]/div[@class=\"col_group\"]/'\n",
    "                                                         'div[@class=\"col col-large\"]/'\n",
    "                                                         'div[@class=\"col col-big js-middle\"]/article/header/'\n",
    "                                                         'time/@datetime').extract()[0]\n",
    "                    title = Selector(html_response).xpath('//body/div[@class=\"layout\"]/div[@class=\"col_group\"]/'\n",
    "                                                          'div[@class=\"col col-large\"]/'\n",
    "                                                          'div[@class=\"col col-big js-middle\"]'\n",
    "                                                          '/article/header/div[@class=\"text\"]/h1').extract()[0]\n",
    "                    title = re.sub(reg_html, '', title).strip()\n",
    "                    lead = Selector(html_response).xpath('//body/div[@class=\"layout\"]/div[@class=\"col_group\"]/'\n",
    "                                                         'div[@class=\"col col-large\"]/'\n",
    "                                                         'div[@class=\"col col-big js-middle\"]/article/'\n",
    "                                                         'div[@class=\"article_text_wrapper\"]/'\n",
    "                                                         'p[@class=\"b-article__text\"]/span').extract()[0]\n",
    "                    lead = re.sub(reg_html, '', lead).strip()\n",
    "                    authors = Selector(html_response).xpath('//body/div[@class=\"layout\"]/div[@class=\"col_group\"]/'\n",
    "                                                            'div[@class=\"col col-large\"]/'\n",
    "                                                            'div[@class=\"col col-big js-middle\"]/article/'\n",
    "                                                            'div[@class=\"article_text_wrapper\"]/'\n",
    "                                                            'p[@class=\"b-article__text document_authors\"]').extract()[0]\n",
    "                    authors = re.sub(reg_html, '', authors).strip()\n",
    "                    meta = ArticleMeta(url_art, time, 'Kommersant, ' + authors, title, lead)\n",
    "                    metas.append(meta)\n",
    "                except:\n",
    "                    pass\n",
    "            print(url + ' is processed')\n",
    "        except:\n",
    "            pass\n",
    "    return metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.kommersant.ru/archive/rubric/40/2017-01-01 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-02 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-03 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-04 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-05 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-06 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-07 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-08 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-09 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-10 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-11 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-12 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-13 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-14 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-15 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-16 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-17 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-18 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-19 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-20 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-21 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-22 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-23 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-24 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-25 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-26 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-27 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-28 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-29 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-30 is processed\n",
      "https://www.kommersant.ru/archive/rubric/40/2017-01-31 is processed\n",
      "2017 ready to be stacked\n",
      "2017 is stacked into CSV\n"
     ]
    }
   ],
   "source": [
    "# Stack data\n",
    "def stack_data(metas, year):\n",
    "    d = {'URL': [], 'Publish Date': [], 'Authors': [], 'Title': [], 'Lead sentences': [],\n",
    "            'Lemmatized lead sentences': [], 'Lemm Analysis': []}\n",
    "    print(str(year) + ' ready to be stacked')\n",
    "    for article in metas:\n",
    "        d['URL'].append(article.url)\n",
    "        d['Publish Date'].append(article.pub_date)\n",
    "        d['Authors'].append(article.authors)\n",
    "        d['Title'].append(article.title)\n",
    "        d['Lead sentences'].append(article.lead)\n",
    "        d['Lemmatized lead sentences'].append(article.sentences)\n",
    "        d['Lemm Analysis'].append(article.analysis)\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df.to_csv('scrape_metadata_' + str(year) + '.csv')\n",
    "    print(str(year) + ' is stacked into CSV')\n",
    "    return(df)\n",
    "\n",
    "\n",
    "\n",
    "def res(year):\n",
    "    stack_data(scrape_archive(year), year)\n",
    "\n",
    "data = res(2017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Lead sentences</th>\n",
       "      <th>Lemm Analysis</th>\n",
       "      <th>Lemmatized lead sentences</th>\n",
       "      <th>Publish Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Kommersant, Юлия Локшина</td>\n",
       "      <td>Вопрос о дальнейшей судьбе банка \"Пересвет\", п...</td>\n",
       "      <td>S,муж,неод=(вин,ед|им,ед) A=(пр,ед,полн,жен|да...</td>\n",
       "      <td>вопрос дальнейший судьба банк пересвет принадл...</td>\n",
       "      <td>2017-01-09T00:00:00+03:00</td>\n",
       "      <td>Санацию \"Пересвета\" поддержал премьер-министр</td>\n",
       "      <td>https://www.kommersant.ru/doc/3186810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Kommersant, Мария Сарычева</td>\n",
       "      <td>Количество сделок слияний и поглощений (M&amp;amp;...</td>\n",
       "      <td>S,сред,неод=(вин,ед|им,ед) S,жен,неод=род,мн S...</td>\n",
       "      <td>количество сделка слияние поглощение M amp A г...</td>\n",
       "      <td>2017-01-09T00:00:00+03:00</td>\n",
       "      <td>ТЭК попал в аутсайдеры</td>\n",
       "      <td>https://www.kommersant.ru/doc/3186841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Kommersant, Виталий Гайдаев</td>\n",
       "      <td>Впервые за четыре года январские валютные торг...</td>\n",
       "      <td>ADV= NUM=(им|вин,неод) S,муж,неод=(вин,мн|род,...</td>\n",
       "      <td>впервые четыре год январский валютный торги ро...</td>\n",
       "      <td>2017-01-09T00:00:00+03:00</td>\n",
       "      <td>Игра на укрепление российской валюты продолжается</td>\n",
       "      <td>https://www.kommersant.ru/doc/3186807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Kommersant, Юлия Полякова, Владислав Новый</td>\n",
       "      <td>ВТБ и АФК \"Система\" договорились о дальнейшей ...</td>\n",
       "      <td>S,сокр,ед,муж,неод=(пр|вин|дат|род|твор|им) S,...</td>\n",
       "      <td>втб АФК система договариваться дальнейший судь...</td>\n",
       "      <td>2017-01-09T00:00:00+03:00</td>\n",
       "      <td>Банк превратился из акционера холдинга в его о...</td>\n",
       "      <td>https://www.kommersant.ru/doc/3186782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Kommersant, Евгений Хвостик</td>\n",
       "      <td>Генеральный директор Лондонской фондовой биржи...</td>\n",
       "      <td>A=(вин,ед,полн,муж,неод|им,ед,полн,муж) S,муж,...</td>\n",
       "      <td>генеральный директор лондонский фондовый биржа...</td>\n",
       "      <td>2017-01-10T21:48:13+03:00</td>\n",
       "      <td>Гендиректор Лондонской фондовой биржи и глава ...</td>\n",
       "      <td>https://www.kommersant.ru/doc/3188014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                     Authors  \\\n",
       "0           0                    Kommersant, Юлия Локшина   \n",
       "1           1                  Kommersant, Мария Сарычева   \n",
       "2           2                 Kommersant, Виталий Гайдаев   \n",
       "3           3  Kommersant, Юлия Полякова, Владислав Новый   \n",
       "4           4                 Kommersant, Евгений Хвостик   \n",
       "\n",
       "                                      Lead sentences  \\\n",
       "0  Вопрос о дальнейшей судьбе банка \"Пересвет\", п...   \n",
       "1  Количество сделок слияний и поглощений (M&amp;...   \n",
       "2  Впервые за четыре года январские валютные торг...   \n",
       "3  ВТБ и АФК \"Система\" договорились о дальнейшей ...   \n",
       "4  Генеральный директор Лондонской фондовой биржи...   \n",
       "\n",
       "                                       Lemm Analysis  \\\n",
       "0  S,муж,неод=(вин,ед|им,ед) A=(пр,ед,полн,жен|да...   \n",
       "1  S,сред,неод=(вин,ед|им,ед) S,жен,неод=род,мн S...   \n",
       "2  ADV= NUM=(им|вин,неод) S,муж,неод=(вин,мн|род,...   \n",
       "3  S,сокр,ед,муж,неод=(пр|вин|дат|род|твор|им) S,...   \n",
       "4  A=(вин,ед,полн,муж,неод|им,ед,полн,муж) S,муж,...   \n",
       "\n",
       "                           Lemmatized lead sentences  \\\n",
       "0  вопрос дальнейший судьба банк пересвет принадл...   \n",
       "1  количество сделка слияние поглощение M amp A г...   \n",
       "2  впервые четыре год январский валютный торги ро...   \n",
       "3  втб АФК система договариваться дальнейший судь...   \n",
       "4  генеральный директор лондонский фондовый биржа...   \n",
       "\n",
       "                Publish Date  \\\n",
       "0  2017-01-09T00:00:00+03:00   \n",
       "1  2017-01-09T00:00:00+03:00   \n",
       "2  2017-01-09T00:00:00+03:00   \n",
       "3  2017-01-09T00:00:00+03:00   \n",
       "4  2017-01-10T21:48:13+03:00   \n",
       "\n",
       "                                               Title  \\\n",
       "0      Санацию \"Пересвета\" поддержал премьер-министр   \n",
       "1                             ТЭК попал в аутсайдеры   \n",
       "2  Игра на укрепление российской валюты продолжается   \n",
       "3  Банк превратился из акционера холдинга в его о...   \n",
       "4  Гендиректор Лондонской фондовой биржи и глава ...   \n",
       "\n",
       "                                     URL  \n",
       "0  https://www.kommersant.ru/doc/3186810  \n",
       "1  https://www.kommersant.ru/doc/3186841  \n",
       "2  https://www.kommersant.ru/doc/3186807  \n",
       "3  https://www.kommersant.ru/doc/3186782  \n",
       "4  https://www.kommersant.ru/doc/3188014  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('scrape_metadata_2017.csv')\n",
    "\n",
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
